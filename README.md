# Project Argus â€” Monorepo

A comprehensive toolkit for extracting, processing, and analyzing Google Maps review data.

## ğŸ”„ Updates

- Bloom filter hashing improved for lower collision rates.
- Autoscaled pool cleans up timers on shutdown and reports memory usage.
- Python ingest validates schema via lightweight NDJSON generator.

## ğŸ—ï¸ Architecture

```text
argus/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ userscript/           # Tampermonkey bundle + build
â”‚   â””â”€â”€ scraper-playwright/   # Playwright-based scraper
â”œâ”€â”€ libs/
â”‚   â””â”€â”€ js-core/             # Shared JavaScript utilities
â”œâ”€â”€ py/
â”‚   â”œâ”€â”€ ingest/              # Data processing and normalization
â”‚   â””â”€â”€ analysis/            # Jupyter notebooks and EDA
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ps/                  # PowerShell utilities
â”œâ”€â”€ specs/                   # JSON schemas and contracts
â”œâ”€â”€ docs/                    # Documentation and ADRs
â””â”€â”€ datasets/                # Local data storage (gitignored)
```

## ğŸš€ Quick Start

### Prerequisites

- **Node.js 20+** with pnpm
- **Python 3.11+** with uv
- **PowerShell 7+** (Windows)

### Setup

1. **Clone and setup:**

   ```bash
   git clone <repository-url>
   cd argus
   pwsh -f scripts/ps/setup.ps1
   ```

2. **Manual setup (alternative):**

   ```bash
   # Node.js dependencies
   pnpm -w install

   # Python environments
   cd py/ingest && uv venv && uv pip sync requirements.txt
   cd ../analysis && uv venv && uv pip sync requirements.txt
   ```

3. **Verify installation:**

   ```bash
   pnpm -w run build
   pnpm -w run lint
   ```

## ğŸ“¦ Workspace Commands

```bash
# Build all packages
pnpm -w run build

# Lint all packages
pnpm -w run lint

# Run tests
pnpm -w run test

# Development mode
pnpm -w run dev
```

## ğŸ”§ Individual Apps

### Userscript

```bash
cd apps/userscript
pnpm run build    # Build userscript bundle
pnpm run dev      # Watch mode
```

### Scraper

```bash
cd apps/scraper-playwright
pnpm run build    # Compile TypeScript
pnpm run start    # Run scraper
```

### Python Processing

```bash
cd py/ingest
uv run python src/processor.py input.json --output-dir datasets
```

## ğŸ§¹ Maintenance

```bash
# Clean development artifacts
pwsh -f scripts/ps/cleanup.ps1

# Repository hardening
pwsh -f scripts/ps/repo-hardening.ps1
```

## ğŸ“Š Data Flow

1. **Extraction**: Userscript or Playwright scraper collects review data
2. **Processing**: Python ingest module normalizes and validates data
3. **Analysis**: Jupyter notebooks for exploratory data analysis
4. **Output**: Parquet/CSV files for further analysis

## ğŸ”’ Security & Compliance

- No sensitive data in repository
- Rate limiting in scrapers
- Respectful of Google Maps ToS
- Local data storage only

## ğŸ¤ Contributing

1. Follow the established monorepo structure
2. Use pnpm workspaces for Node.js packages
3. Use uv for Python dependency management
4. Run `pnpm -w run lint` before committing
5. Update documentation for API changes

## ğŸ“ License

MIT License - Project Argus
